from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import pandas as pd
import numpy as np
from collections import deque
from cnn_lstm_hybrid_model.cnn_lstm_pipeline import CNNLSTMPipeline
import uvicorn

class SensorData(BaseModel):
    x: float
    y: float
    z: float

app = FastAPI(title="Real-time Posture API")

# Model yÃ¼kleme
pipeline = CNNLSTMPipeline(window_size=20)
try:
    pipeline.load_pipeline("pipeline")
    print("âœ… Model yÃ¼klendi")
except Exception as e:
    print(f"âš ï¸ Model yÃ¼klenemedi: {e}")
    print("Yeniden eÄŸitiliyor...")
    df = pd.read_csv("cnn_lstm_hybrid_model/datasets/new_dataset.csv")
    pipeline.fit(df, epochs=30, batch_size=32)
    pipeline.save_pipeline("pipeline")
    print("âœ… Model eÄŸitildi ve kaydedildi")

# Sliding window buffer (API seviyesinde)
WINDOW_SIZE = 20
data_buffer = deque(maxlen=WINDOW_SIZE)

@app.get("/")
def root():
    return {
        "status": "running",
        "buffer_size": len(data_buffer),
        "window_size": WINDOW_SIZE,
        "pipeline_buffer": len(pipeline.data_buffer)
    }

@app.post("/predict")
def predict(data: SensorData):
    """
    GerÃ§ek zamanlÄ± tahmin - Sliding window
    Pipeline'Ä±n kendi buffer'Ä±nÄ± kullanmayÄ±p API seviyesinde yÃ¶netiyoruz
    """
    try:
        # Veriyi buffer'a ekle (otomatik olarak en eski veri silinir)
        data_buffer.append([data.x, data.y, data.z])
        
        current_size = len(data_buffer)
        
        # Ä°lk 20 veri toplanana kadar bekleme
        if current_size < WINDOW_SIZE:
            return {
                "status": "collecting",
                "message": f"Kalibrasyon ({current_size}/{WINDOW_SIZE})",
                "buffer_size": current_size
            }
        
        # Buffer doldu - Tahmin yap
        # Buffer'Ä± numpy array'e Ã§evir
        window_data = np.array(data_buffer)  # Shape: (20, 3)
        
        # Pipeline'Ä±n model'ini direkt kullan
        window_data_reshaped = window_data.reshape(1, WINDOW_SIZE, 3)
        
        # Tahmin
        prediction = pipeline.model.predict(window_data_reshaped, verbose=0)
        predicted_class = np.argmax(prediction[0])
        confidence = float(prediction[0][predicted_class])
        
        # Label'a Ã§evir
        posture = pipeline.label_encoder.inverse_transform([predicted_class])[0]
        
        # Debug: TÃ¼m sÄ±nÄ±flarÄ±n olasÄ±lÄ±klarÄ±nÄ± gÃ¶ster
        all_predictions = {
            pipeline.label_encoder.inverse_transform([i])[0]: round(float(prediction[0][i]), 3)
            for i in range(len(prediction[0]))
        }
        
        print(f"ğŸ“Š Tahmin DaÄŸÄ±lÄ±mÄ±: {all_predictions}")
        print(f"ğŸ“ Buffer son 3 veri: {list(data_buffer)[-3:]}")
        
        return {
            "status": "ok",
            "posture": posture,
            "confidence": round(confidence, 3),
            "buffer_size": current_size,
            "all_predictions": all_predictions  # Debug iÃ§in
        }
        
    except Exception as e:
        print(f"âŒ Hata: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/reset")
def reset_buffer():
    """Buffer'Ä± sÄ±fÄ±rla"""
    data_buffer.clear()
    pipeline.reset_buffer()  # Pipeline'Ä±n kendi buffer'Ä±nÄ± da temizle
    return {
        "status": "success",
        "message": "Buffer sÄ±fÄ±rlandÄ±",
        "buffer_size": 0
    }

@app.get("/status")
def get_status():
    """Sistem durumu"""
    return {
        "api_buffer_size": len(data_buffer),
        "pipeline_buffer_size": len(pipeline.data_buffer),
        "window_size": WINDOW_SIZE,
        "model_loaded": pipeline.model is not None,
        "classes": list(pipeline.label_encoder.classes_) if hasattr(pipeline.label_encoder, 'classes_') else []
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)